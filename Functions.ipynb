{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2519b4-b88d-4297-81d2-db19586fa36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from datetime import date, timedelta\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.style as style\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.pylab import rcParams\n",
    "import seaborn as sns\n",
    "import locale\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery\n",
    "import time\n",
    "import unicodedata\n",
    "import missingno as msno\n",
    "import sweetviz as sv\n",
    "from ydata_profiling import ProfileReport\n",
    "from fuzzywuzzy import fuzz\n",
    "from wordcloud import WordCloud\n",
    "import requests\n",
    "import random\n",
    "import itertools\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import interpolate\n",
    "from itertools import combinations\n",
    "from plotly import graph_objects as go\n",
    "import string\n",
    "import pycountry\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219dbbfe-d260-4be8-9fc7-017910abb71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify Excel files (within notebook's folder)\n",
    "\n",
    "def excel_files():\n",
    "    current_dir = os.getcwd()\n",
    "    files = [file for file in os.listdir(current_dir) if file.endswith('.xlsx') or file.endswith('.csv')]\n",
    "\n",
    "    print(f'folder name = {current_dir}\\n')\n",
    "    print('\\033[1mExcel files within the folder :\\033[0m')\n",
    "    print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4498944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced value_counts\n",
    "\n",
    "def value_counts_2(df, col_name, c_map = 'Blues', drop_null = False):\n",
    "    \n",
    "    x = df[col_name].value_counts(dropna = drop_null).reset_index()\n",
    "    x.columns = [col_name.upper(), 'Count']\n",
    "    x['% Total'] = (x['Count'] / x['Count'].sum()) * 100\n",
    "    x.set_index(col_name.upper(), inplace = True)\n",
    "\n",
    "    print(f\"Total values = {x['Count'].sum():,}\")\n",
    "    \n",
    "    return x.style.background_gradient(subset = '% Total', cmap = c_map).format(precision = 2)\n",
    "\n",
    "#vc = value_counts_2(df,'status')\n",
    "#vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1561c8ec-cf9a-4efa-9044-4ab47554fa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datetime columns\n",
    "\n",
    "def datetime_cols(df, datetime_col, datetime_format = 'YYYY-MM-DD', year = True, month = True, week = True):\n",
    "\n",
    "    df[datetime_col] = pd.to_datetime(df[datetime_col], format = datetime_format)\n",
    "\n",
    "    if year == True:\n",
    "        df['year'] = pd.to_datetime(df[datetime_col]).dt.to_period('Y')\n",
    "    if month == True:\n",
    "        df['month'] = pd.to_datetime(df[datetime_col]).dt.to_period('M')\n",
    "    if week == True:\n",
    "        df['week'] = df[datetime_col].dt.to_period('W').apply(lambda x: x.start_time)\n",
    "        df['week'] = df['week'].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b510be-e3e7-49ca-9bf6-eddabbb70f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate Values\n",
    "\n",
    "def consolidate_values(df, col_name, threshold = 1, other_value = 'Other', consolidate_null = True):\n",
    "\n",
    "    a = df.copy()\n",
    "    pop_counts = a[col_name].value_counts(dropna = not consolidate_null)\n",
    "    total_pop = pop_counts.sum()\n",
    "    threshold_value = total_pop * threshold / 100\n",
    "    consolidated_values = pop_counts[pop_counts < threshold_value].index\n",
    "    a[col_name] = a[col_name].apply(lambda x: other_value if x in consolidated_values else x)\n",
    "    if consolidate_null:\n",
    "        a[col_name] = a[col_name].fillna(other_value)\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04c363b-e182-429a-8562-4adf622b1d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BigQuery connection\n",
    "\n",
    "def query(query, project_name = 'rapyd-bq-poc-2020'):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    client = bigquery.Client(project = project_name)\n",
    "    query_job = client.query(query)\n",
    "    df = query_job.to_dataframe()\n",
    "\n",
    "    seconds = round(time.time() - start_time, 0)\n",
    "    print(f'Execution time = {str(timedelta(seconds = seconds))} minutes')\n",
    "    print('------------------------------------')\n",
    "    print(f'Rows = {df.shape[0]:,} | Columns = {df.shape[1]}'), print('')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cdbc54-729e-46e7-a0a8-518e9ec5e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Accented Characters \n",
    "\n",
    "def replace_accents(text):\n",
    "    return ''.join(char for char in unicodedata.normalize('NFD',str(text)) if not unicodedata.combining(char))\n",
    "\n",
    "#df = df.applymap(replace_accents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f82e1d-461d-4f3c-8d74-58f51a39a094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Crosstab\n",
    "\n",
    "def advanced_crosstab(df, row, col, normalize_by = 'row', change = 'drop', chart_size = (10,6), \\\n",
    "                 cmap = 'Oranges', line = 'white', text = 8, rotation = 0):\n",
    "    \n",
    "    if normalize_by not in ['row', 'col', 'all']:\n",
    "        raise ValueError(f'Expected \"normalize_by\" param to be either \"row\", \"col\" or \"all\". Got \"{normalize_by}\" instead.')\n",
    "    if change not in ['abs_diff', 'drop']:\n",
    "        raise ValueError(f'Expected \"change\" param to be either \"abs_diff\" or \"drop\". Got \"{change}\" instead.')\n",
    "\n",
    "    df2 = df.copy()\n",
    "    df2[row].fillna('-', inplace = True)\n",
    "    df2[col].fillna('-', inplace = True)\n",
    "\n",
    "    norm = {'row':0, 'col':1, 'all':True}.get(normalize_by)\n",
    "    ct1 = pd.crosstab(df2[row], df2[col], normalize = norm)\n",
    "    ct2 = pd.crosstab(df2[row], df2[col], margins = True, margins_name = 'Total')\n",
    "    cross = ct1.combine_first(ct2)\n",
    "\n",
    "    # push the total column & row to the end\n",
    "    total_col = cross.pop('Total')\n",
    "    cross['Total'] = total_col\n",
    "    cross.sort_values(by = 'Total', ascending = False, inplace = True)\n",
    "\n",
    "    total_row = cross.loc['Total']\n",
    "    cross.drop('Total', axis = 0, inplace = True)\n",
    "    cross.loc['Total'] = total_row\n",
    "\n",
    "    if change == 'abs_diff':\n",
    "        cross['absolute diff'] = cross['Total'].diff().fillna(0).astype(int)\n",
    "        cross['absolute diff'].iloc[-1] = 0\n",
    "\n",
    "    num = -1 if change == 'drop' else -2\n",
    "    mask_1 = np.zeros(cross.shape)\n",
    "    mask_1[:, num:] = True\n",
    "    mask_1[-1, :] = True\n",
    "    mask_2 = np.zeros(cross.shape)\n",
    "    mask_2[:-1, :num] = True\n",
    "\n",
    "    plt.figure(figsize = chart_size)\n",
    "    plt.rcParams.update({'font.size':text})\n",
    "    ax = sns.heatmap(cross,mask=mask_1,cbar=False,cmap=cmap,annot=True,fmt='.2%',annot_kws={'color':'black'},lw=2,linecolor=line)\n",
    "    ax = sns.heatmap(cross,mask=mask_2,cbar=False,alpha=0,annot=True,fmt='.0f',annot_kws={'color':'black'},lw=2)  \n",
    "\n",
    "    plt.xlabel(col.replace('_',' ').title()), plt.ylabel(row.replace('_',' ').title())\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    ax.tick_params(axis = 'x', which = 'major', rotation = rotation)\n",
    "    ax.tick_params(axis = 'y', which = 'major', rotation = rotation)\n",
    "    ax.set_xlabel(col.replace('_',' ').title(), fontweight = 'bold', labelpad = 15)\n",
    "    ax.set_ylabel(row.replace('_',' ').title(), fontweight = 'bold', labelpad = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7b3580-6792-4f5e-bca7-f61f16647e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a Mail\n",
    "\n",
    "import smtplib \n",
    "from email.mime.multipart import MIMEMultipart \n",
    "from email.mime.text import MIMEText \n",
    "from email.mime.base import MIMEBase \n",
    "from email import encoders \n",
    "\n",
    "csv_file = '/Users/itaymi/Desktop/Python/gmail_pass.csv'\n",
    "\n",
    "def gmail(recipients, subject, email_message, files = [], csv_pass = csv_file):\n",
    "\n",
    "    g = pd.read_csv(csv_file)\n",
    "    sender, password = g.at[0,'gmail'], g.at[1,'gmail']\n",
    "\n",
    "    msg = MIMEMultipart()  \n",
    "    msg['From'] = sender  \n",
    "    msg['To'] = recipients \n",
    "    msg['Subject'] = subject\n",
    "    msg.attach(MIMEText(email_message, 'plain')) \n",
    "\n",
    "    for file in files:\n",
    "        filename = file\n",
    "        attachment = open(file, 'rb') \n",
    "        p1 = MIMEBase('application', 'octet-stream')  \n",
    "        p1.set_payload((attachment).read()) \n",
    "        encoders.encode_base64(p1) \n",
    "        p1.add_header('Content-Disposition', 'attachment ; filename = %s' % filename) \n",
    "        msg.attach(p1) \n",
    "\n",
    "    server = smtplib.SMTP('smtp.gmail.com', 587) \n",
    "    server.starttls() \n",
    "    server.login(sender, password)\n",
    "    server.sendmail(sender, recipients.split(','), msg.as_string())\n",
    "\n",
    "    print('Mail sent successfully :)')\n",
    "    \n",
    "    server.quit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbea247-300f-4cff-a561-9a1aee8d5c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "\n",
    "def missing_values(df, fig_size = (16,8), w = [1,1], h = [1,1], ws = 0.3, hs = 0.8):\n",
    "\n",
    "    print('\\033[1mMissing values (%):\\033[0m')\n",
    "    print(round((df.isna().sum() / len(df)) * 100, 2))\n",
    "\n",
    "    fig = plt.figure(figsize = fig_size)\n",
    "    grid = GridSpec(2, 2, width_ratios = w, height_ratios = h, wspace = ws, hspace = hs)\n",
    "    \n",
    "    ax1 = plt.subplot(grid[0,0])\n",
    "    msno.bar(df, fontsize = 8, color = 'dodgerblue', ax = ax1)\n",
    "    ax1.set_title('Bar Chart (existing values)', fontsize = 10)\n",
    "    \n",
    "    ax2 = plt.subplot(grid[0,1])\n",
    "    msno.heatmap(df, fontsize = 8, ax = ax2)\n",
    "    ax2.set_title('Heatmap', fontsize = 10)\n",
    "    \n",
    "    ax3 = plt.subplot(grid[1,0])\n",
    "    msno.dendrogram(df, fontsize = 8, ax = ax3)\n",
    "    ax3.set_title('Dendrogram', fontsize = 10)\n",
    "    \n",
    "    ax4 = plt.subplot(grid[1,1])\n",
    "    msno.matrix(df, fontsize = 8, ax = ax4)\n",
    "    ax4.set_title('Matrix', fontsize = 10)\n",
    "    \n",
    "    fig.suptitle('Missing Data Patterns')\n",
    "    fig.subplots_adjust(top = 0.85)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a636a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare 2 dataframes\n",
    "\n",
    "def compare_dataframes(df1,df2):\n",
    "    \n",
    "    def organize_df(df):\n",
    "        df.sort_values(by = df.columns.tolist(), ignore_index = True, inplace = True)\n",
    "        duplicates = df.duplicated(keep = False).astype(int)\n",
    "        df['is_duplicate'] = duplicates\n",
    "        \n",
    "    organize_df(df1)\n",
    "    organize_df(df2)\n",
    "    \n",
    "    # merged dataframe\n",
    "    df3 = df1.iloc[:,:-1].merge(df2.iloc[:,:-1], how = 'outer')\n",
    "    df3.drop_duplicates(ignore_index = True, inplace = True)\n",
    "    \n",
    "    # add df1 flag\n",
    "    df3 = df3.merge(df1, how = 'left')\n",
    "    df3.rename(columns = {'is_duplicate':'exists_df1'}, inplace = True)\n",
    "    df3['exists_df1'] = df3['exists_df1'].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "    \n",
    "    # add df2 flag\n",
    "    df3 = df3.merge(df2, how = 'left')\n",
    "    df3.rename(columns = {'is_duplicate':'exists_df2'}, inplace = True)\n",
    "    df3['exists_df2'] = df3['exists_df2'].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "    \n",
    "    # flag errors\n",
    "    df3['error'] = df3.apply(lambda row: 0 if row['exists_df1']+row['exists_df2'] == 2 else 1, axis = 1)\n",
    "\n",
    "    print(f'\\033[1m1st df length = \\033[0m{len(df1):,} (duplications = {df1.is_duplicate.sum():,})')\n",
    "    print(f'\\033[1m2nd df length = \\033[0m{len(df2):,} (duplications = {df2.is_duplicate.sum():,})')\n",
    "    print(f'\\033[1mNo. of errors = \\033[0m{df3.error.sum():,}')\n",
    "    \n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be4e956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned number format\n",
    "\n",
    "def format_fixer(df,cols):\n",
    "    for col in cols:\n",
    "        df[col] = df[col].apply(lambda x: ''.join([c if c.isdigit() or c == '.' else '' for c in str(x)]))\n",
    "        df[col] = pd.to_numeric(df[col], errors = 'coerce')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89701ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column to string\n",
    "\n",
    "def column_to_string(df, col, values_type = 'str'):\n",
    "    \n",
    "    a = df.loc[:,[col]].copy()\n",
    "    a.drop_duplicates(subset = col, inplace = True)\n",
    "    \n",
    "    if values_type == 'str':\n",
    "        a[col] = \"'\" + a[col].astype(str) + \"'\"\n",
    "    \n",
    "    b = a[col].to_numpy()\n",
    "    b = ','.join(str(x) for x in b) \n",
    "    \n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a8a1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retention_matrix(df, \n",
    "                     cohort_col = 'trx_month', \n",
    "                     cohort_name = '1st transaction', \n",
    "                     cmap = 'Purples',\n",
    "                     title_name = 'Retention Matrix', \n",
    "                     x_label = 'Months since 1st transaction', \n",
    "                     y_label = '1st transaction month',\n",
    "                     figsize = (15,10), \n",
    "                     font_size = 8, \n",
    "                     months_to_exclude = 0, \n",
    "                     fmt = '.1%'):\n",
    "\n",
    "    plt.rcParams.update({'figure.figsize':figsize, 'font.size':font_size})\n",
    "\n",
    "    a = df.copy()\n",
    "\n",
    "    # assign cohort   \n",
    "    a[cohort_col] = pd.to_datetime(a[cohort_col])\n",
    "    first = a.groupby('merchant_id', as_index = False)[cohort_col].min()\n",
    "    first.columns = ['merchant_id','start']\n",
    "\n",
    "    # retention dataframe\n",
    "    retention = a.merge(first, on = 'merchant_id', how = 'inner')\n",
    "    retention['period_diff'] = ((retention[cohort_col] - retention['start']) / np.timedelta64(1, 'M'))\n",
    "    retention['period_diff'] = np.round(retention['period_diff'],0).fillna(-1).astype(int)\n",
    "    \n",
    "    # cohort size\n",
    "    cohort_size = retention.groupby('start').merchant_id.nunique().sort_index(ascending = False).reset_index()\n",
    "    diff = pd.crosstab(retention['start'], retention['period_diff']).apply(pd.to_numeric, errors = 'coerce').reset_index()\n",
    "\n",
    "    # merge cohort size & retention dataframes\n",
    "    x = cohort_size.merge(diff, on = 'start', how = 'left')\n",
    "    x.rename(columns = {'start':cohort_name,'merchant_id':'Merchants'}, inplace = True)  \n",
    "\n",
    "    # drop historical months\n",
    "    if months_to_exclude > 0:\n",
    "        x = x.iloc[:-months_to_exclude,:-months_to_exclude].copy()\n",
    "\n",
    "    # add zero colomuns if needed (cubic dataframe)\n",
    "    int_list = x.columns[2:].astype(int)\n",
    "    for i, j in enumerate(x):\n",
    "        if not i in int_list:\n",
    "            x[i] = 0\n",
    "    x = x.iloc[1:, :-2].copy()\n",
    "\n",
    "    # columns reorder\n",
    "    cols_1 = [cohort_name, 'Merchants']\n",
    "    cols_2 = list(range(int(x.columns[2:].max()+1)))\n",
    "    cols = cols_1 + cols_2\n",
    "    x = x[cols]\n",
    "\n",
    "    # convert to percentages\n",
    "    cohort_sizes = x.loc[:,'Merchants']\n",
    "    rate = x.iloc[:,1:].divide(cohort_sizes, axis = 0)\n",
    "    rate = rate.iloc[:,1:].copy()\n",
    "    rate.columns = rate.columns.astype(int)\n",
    "\n",
    "    # merge to the final dataframe\n",
    "    final = pd.concat([x.loc[:,cols_1], rate], axis = 1)\n",
    "    final[cohort_name] = pd.to_datetime(final[cohort_name]).dt.strftime('%y-%b')\n",
    "    final.drop(0, axis = 1, inplace = True)\n",
    "\n",
    "    # reshape the dataframe to fit the heatmap visualization\n",
    "    final.set_index(cohort_name, inplace = True)   \n",
    "    mask_1 = np.zeros(final.shape)\n",
    "    final[final == 0] = np.nan\n",
    "    mask_1[:, 0] = True\n",
    "    mask_2 = np.zeros(final.shape)\n",
    "    mask_2[:, 1:] = True\n",
    "\n",
    "    # visualization\n",
    "    ax = sns.heatmap(final, mask = mask_1, cbar = False, cmap = cmap, alpha = 0.6, annot = True, fmt = fmt, lw = 2, annot_kws = {'color':'black'})\n",
    "    ax = sns.heatmap(final, mask = mask_2, cbar = False, alpha = 0, annot = True, fmt = ',.0f', lw = 2, annot_kws = {'color':'black'})  \n",
    "\n",
    "    # final touch ups\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    ax.tick_params(axis = 'both', which = 'major', labelsize = font_size, rotation = 0)\n",
    "    plt.title(title_name, weight = 'bold', pad = 15, fontsize = font_size + 6)\n",
    "    plt.xlabel(x_label, labelpad = 18, fontsize = font_size + 2)\n",
    "    plt.ylabel(y_label, labelpad = 18, fontsize = font_size + 2)\n",
    "    plt.show()\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5218529-06a3-4a96-be67-8aa4dc6b1a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cartesian function\n",
    "\n",
    "def add_cartesian(df, group_col = 'merchant_id', date_col = 'created_date', value_col = 'num_trx'):\n",
    "    \n",
    "    # add new columns\n",
    "    a = df.copy()\n",
    "    a[date_col] = pd.to_datetime(a[date_col])\n",
    "    a['month'] = pd.to_datetime(a[date_col].dt.strftime('%Y-%m-01'))    \n",
    "    a['cohort'] = pd.to_datetime(a.groupby(group_col)[date_col].transform(lambda x: x.min().replace(day = 1)))\n",
    "    \n",
    "    # months' dataframe\n",
    "    months = pd.DataFrame(pd.date_range(start = a.month.min(), end = a.month.max(), freq = 'MS'), \n",
    "                          columns = ['month'])\n",
    "\n",
    "    # monthly per user\n",
    "    users = a.groupby([group_col,'month'], as_index = False)[value_col].sum()\n",
    "    \n",
    "    # Cartesian product of user_id & month\n",
    "    cross = pd.MultiIndex.from_product([a[group_col].unique(), months.month.unique()], \n",
    "                                       names = [group_col,'month']).to_frame(index = False)\n",
    "\n",
    "    # 1st merge\n",
    "    x = cross.merge(users, on = [group_col,'month'], how = 'left')\n",
    "    \n",
    "    # 2nd merge (add cohort)\n",
    "    x = x.merge(a.groupby(group_col, as_index = False).cohort.min(), on = group_col, how = 'inner')\n",
    "    \n",
    "    x[value_col] = x[value_col].fillna(0)\n",
    "    x['month_diff'] = ((x['month'] - x['cohort']) / np.timedelta64(1,'M'))\n",
    "    x['month_diff'] = np.round(x['month_diff']).fillna(-1).astype(int)\n",
    "    \n",
    "    # reorder columns\n",
    "    x = x.loc[:,[group_col,'cohort','month','month_diff',value_col]].copy()\n",
    "    \n",
    "    # sort & drop redundant rows\n",
    "    x = x[x.month >= x.cohort].copy()\n",
    "    x.sort_values(by = [group_col,'month'], ignore_index = True, inplace = True)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6055702c-6bd1-41cb-a0c1-6ed399048a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Prediction\n",
    "\n",
    "def linear_prediction(df, pred_col, group_col, month_col = 'month', months = 3):\n",
    "\n",
    "    # convert to datetime and sort df\n",
    "    x = df.copy()\n",
    "    x[month_col] = pd.to_datetime(x[month_col])\n",
    "    x.sort_values([group_col,month_col], inplace = True)\n",
    "\n",
    "    # calculate the linear prediction\n",
    "    x[f'linear_pred_{months}m'] = x.groupby(group_col)[pred_col].shift(1).rolling(window = months, min_periods = months).\\\n",
    "                                  apply(lambda x: LinearRegression().fit(pd.Series(range(len(x))).values.reshape(-1,1), \\\n",
    "                                  x.values.reshape(-1,1)).predict([[months]])[0][0]).round(2).reset_index(level = 0, drop = True)\n",
    "\n",
    "    x[f'slope_{months}m'] = x.groupby(group_col)[pred_col].shift(1).rolling(window = months, min_periods = months).\\\n",
    "                            apply(lambda x: LinearRegression().fit(pd.Series(range(len(x))).values.reshape(-1,1), \\\n",
    "                            x.values.reshape(-1,1)).coef_[0][0]).round(2).reset_index(level = 0, drop = True)\n",
    "                           \n",
    "    # convert back to string\n",
    "    x[month_col] = x[month_col].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5130cb3-d223-43e9-8357-136bcffa3ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_group_column(df, columns):\n",
    "\n",
    "    final = pd.DataFrame()\n",
    "    counter = 0\n",
    "    lists = [df[col].unique().tolist() for col in columns]\n",
    "    \n",
    "    print('\\033[1mUnique values :\\033[0m')\n",
    "    print(lists), print('')\n",
    "    print(f'\\033[1mPossible combinations = \\033[0m{len(list(itertools.product(*lists))):,}')\n",
    "\n",
    "    for combination in list(itertools.product(*lists)):\n",
    "        temp = df.copy()\n",
    "\n",
    "        for i, _ in enumerate(columns):\n",
    "            temp = temp[temp[columns[i]] == combination[i]].copy()\n",
    "            temp['group_name'] = ' | '.join(map(str, combination))\n",
    "\n",
    "            final = pd.concat([final,temp], ignore_index = True)\n",
    "        counter += 1 if len(temp) > 0 else 0\n",
    "\n",
    "    final.sort_values(by = columns, ignore_index = True, inplace = True)\n",
    "    print(f\"\\033[1mPopulated combinations = \\033[0m{final['group_name'].nunique():,}\")\n",
    "    print('')\n",
    "\n",
    "    return final\n",
    "\n",
    "#x = add_group_column(df, ['gender','category','payment_method'])\n",
    "#x.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf981cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to Choose the Number of Clusters:\n",
    "# https://www.youtube.com/watch?v=FqIGui0rwh4\n",
    "\n",
    "def elbow_plot(df, col_name, num_clusters = 10):\n",
    "\n",
    "    rcParams['figure.figsize'] = 10,5\n",
    "\n",
    "    values_reshape = df.loc[:,col_name].values.reshape(-1, 1)\n",
    "\n",
    "    wcss = []\n",
    "    for i in range(1, num_clusters+1):\n",
    "        kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n",
    "        kmeans.fit(values_reshape)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "        \n",
    "    plt.plot(range(1, num_clusters+1), wcss, marker = 'o', ms = 10, lw = 1)\n",
    "    plt.title('The Elbow Method', weight = 'bold')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('WCSS')\n",
    "    plt.gca().xaxis.set_major_locator(plt.MultipleLocator(1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7475eac9-1ac3-4e37-9ca7-46e325f8cc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means \n",
    "\n",
    "def k_means(df, column, num_clusters = 5, quan = .99, name = ''):\n",
    "\n",
    "    # exclude outliers from the algorithm\n",
    "    if quan is not None:\n",
    "        threshold = df[column].quantile(quan)\n",
    "        new = df[df[column] < threshold].copy()\n",
    "        outliers = df[df[column] >= threshold].copy()\n",
    "    else:\n",
    "        new = df.copy()\n",
    "\n",
    "    # execute K-means\n",
    "    kmeans = KMeans(n_clusters = num_clusters, init = 'k-means++', random_state = 42)\n",
    "    values_reshape = new.loc[:,column].values.reshape(-1,1)\n",
    "    y_kmeans = kmeans.fit_predict(values_reshape)\n",
    "\n",
    "    values = new.loc[:,column].copy()\n",
    "    temp = pd.DataFrame({'y_kmeans':y_kmeans, 'values':values})\n",
    "\n",
    "    # assign borders\n",
    "    borders = []\n",
    "    for i in range(num_clusters):\n",
    "        min_border = temp[temp.y_kmeans == i]['values'].min()\n",
    "        borders = np.append(borders, min_border)\n",
    "    borders.sort()\n",
    "\n",
    "    new_col = 'cluster' if name == '' else f'{name}_cluster'\n",
    "    new[new_col] = None\n",
    "    for i, v in enumerate(borders, start = 1):\n",
    "        new[new_col] = np.where(new[column] >= v, i, new[new_col])\n",
    "        \n",
    "    # add outliers    \n",
    "    if quan is not None:\n",
    "        outliers[new_col] = num_clusters\n",
    "        new = pd.concat([new,outliers], ignore_index = True)\n",
    "\n",
    "    print(new.groupby(new_col)[column].agg(['count','min','max']))     \n",
    "\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1107854e-340f-493b-9ae5-19c9ff104adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby with array\n",
    "\n",
    "def group_with_array(df, groupby_col, array_col):\n",
    "\n",
    "    def to_array(x):\n",
    "        if len(x) > 1:\n",
    "            x = np.unique(x)\n",
    "            x.sort()\n",
    "        else:\n",
    "            x = np.array(x)\n",
    "        return x\n",
    "\n",
    "    def array_to_string(arr):\n",
    "        return ', '.join(map(str, arr))\n",
    "        \n",
    "    # generate the new structure    \n",
    "    final = df.groupby(groupby_col)[array_col].agg(to_array).reset_index().reindex([groupby_col,array_col], axis = 1)\n",
    "    final[f'{array_col}_str'] = final[array_col].apply(array_to_string)\n",
    "    final['num_values'] = final[array_col].str.len().fillna(0).astype(int)\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efde2f0-75bf-4251-a614-453dfa0bf691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get quantiles\n",
    "\n",
    "def get_quantile(df, groupby_cols, quantile_col, num_quantiles = 10, print_summary = True):\n",
    "\n",
    "    final = pd.DataFrame()\n",
    "    new_col = quantile_col + ' | quantile'\n",
    "\n",
    "    lists = [df[col].unique().tolist() for col in groupby_cols]\n",
    "\n",
    "    for combination in list(itertools.product(*lists)):\n",
    "        t = df.copy()\n",
    "\n",
    "        for i, _ in enumerate(groupby_cols):\n",
    "            t = t[t[groupby_cols[i]] == combination[i]].copy()\n",
    "\n",
    "        if len(t[quantile_col]) <= num_quantiles:\n",
    "            t[new_col] = 1\n",
    "        else:\n",
    "            t[new_col] = (pd.qcut(t[quantile_col], num_quantiles, labels = False, duplicates = 'drop') + 1)\n",
    "\n",
    "        final = pd.concat([final,t], ignore_index = True)\n",
    "\n",
    "    final.sort_values(by = groupby_cols + [new_col], ignore_index = True, inplace = True)\n",
    "    \n",
    "    if print_summary == True:\n",
    "        print(final.groupby(groupby_cols + [new_col])[quantile_col].agg(['count','min','max','mean','median']).round(2))\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3276e337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text similarity score (\"fuzzywuzzy\")\n",
    "\n",
    "strings_to_drop = ['co','pte','ltd','inc','gmbh','limited','financial','consulting',\n",
    "                   'group','global','digital','international','securities','technology',\n",
    "                   'technologies','previous'] + list(string.punctuation)\n",
    "\n",
    "def calc_similarity(row, col1, col2):\n",
    "    \n",
    "    def clean_string(s):\n",
    "        if strings_to_drop:\n",
    "            for string_to_drop in strings_to_drop:\n",
    "                s = s.replace(string_to_drop, '').strip().lower()\n",
    "                s = replace_accents(s) \n",
    "        return s\n",
    "    \n",
    "    cleaned_col1 = clean_string(row[col1])\n",
    "    cleaned_col2 = clean_string(row[col2])\n",
    "\n",
    "    if pd.isna(cleaned_col1) or pd.isna(cleaned_col2):\n",
    "        return 0\n",
    "    else:\n",
    "        return fuzz.ratio(cleaned_col1, cleaned_col2)\n",
    "    \n",
    "#df['similarity'] = df.apply(calc_similarity, axis = 1, args = (col1, col2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a043eaa-a46b-4573-8325-bdce5db7e2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_jira(from_date, to_date, jira_url, jira_username, jira_api_token):\n",
    "\n",
    "    def adjust_column(df, col_name = 'reporter'):\n",
    "        df[col_name] = df[col_name].str.title().str.strip()\n",
    "        df = df.applymap(replace_accents)\n",
    "\n",
    "    session = requests.Session()\n",
    "    session.auth = (jira_username, jira_api_token)\n",
    "    \n",
    "    start_at, max_results = 0, 100\n",
    "    issues = []\n",
    "    \n",
    "    while True:\n",
    "        jql_query = f'project = DATA AND created >= {from_date} AND created <= {to_date} ORDER BY created DESC'\n",
    "        \n",
    "        params = {'jql':jql_query, 'startAt':start_at, 'maxResults':max_results}\n",
    "        \n",
    "        response = session.get(f'{jira_url}search', params = params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            batch_issues = response.json()['issues']\n",
    "            issues.extend(batch_issues) \n",
    "    \n",
    "            if len(batch_issues) < max_results:\n",
    "                break\n",
    "    \n",
    "            start_at += max_results\n",
    "        else:\n",
    "            print(f'Error: {response.status_code} - {response.text}')\n",
    "            break\n",
    "    \n",
    "    key,summary,status,created,sprint,reporter,email,assignee,team,rapyd_bu,department,story_points,issue_type = [],[],[],[],[],[],[],[],[],[],[],[],[]\n",
    "    \n",
    "    for issue in issues:\n",
    "        key.append(issue['key'])\n",
    "        summary.append(issue['fields']['summary'])\n",
    "        status.append(issue['fields']['status']['name'])\n",
    "        created.append(issue['fields']['created'])\n",
    "        sprint.append(issue['fields'].get('customfield_10113', None))\n",
    "        reporter.append(issue['fields']['reporter']['displayName'] if issue['fields']['reporter'] else None)\n",
    "        email.append(issue['fields']['reporter']['emailAddress'] if issue['fields']['reporter'] else None)\n",
    "        assignee.append(issue['fields']['assignee']['displayName'] if issue['fields']['assignee'] else None)\n",
    "        team.append(issue['fields']['customfield_10703']['value'] if issue['fields']['customfield_10703'] else None)\n",
    "        rapyd_bu.append(issue['fields']['customfield_11671']['value'] if issue['fields']['customfield_11671'] else None)\n",
    "        department.append(issue['fields']['customfield_10623']['value'] if issue['fields']['customfield_10623'] else None)\n",
    "        story_points.append(issue['fields'].get('customfield_10595', None))\n",
    "        issue_type.append(issue['fields']['issuetype']['name'])\n",
    "    \n",
    "    data = {'issue_key':key, 'summary':summary, 'status':status, 'created':created, 'sprint':sprint,\n",
    "            'reporter':reporter, 'email':email, 'assignee':assignee, 'team':team, 'business_unit':rapyd_bu, \n",
    "            'department':department, 'story_points':story_points, 'issue_type': issue_type}\n",
    "    \n",
    "    jira = pd.DataFrame(data)\n",
    "    \n",
    "    jira['created'] = pd.to_datetime(jira['created'].str.split('T').str[0] + ' 00:00:00')\n",
    "    jira['month'] = pd.to_datetime(jira.created).dt.to_period('M')\n",
    "\n",
    "    jira['sprint'] = jira['sprint'].apply(lambda x: x[0]['name'] if isinstance(x,list) and len(x) > 0 and 'name' in x[0] else None)\n",
    "\n",
    "    #jira['bi_n_analytics_sprint'] = jira.analytics_sprint.str.extract(r'(\\D*)(\\d.*)')[1].str.replace(' ','')\n",
    "    \n",
    "    jira['story_points'] = pd.to_numeric(jira['story_points'])\n",
    "    adjust_column(jira)\n",
    "\n",
    "    print(f'Dates: {from_date} until {to_date}')\n",
    "    print(f'No. of tickets = {jira.issue_key.count():,}')\n",
    "    print('')\n",
    "\n",
    "    return jira"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8077642-6457-4631-8bc0-63fa96585eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_plot(df, col, hue, figsize = (15,3), fontsize = 8, xticks_rot = 90):\n",
    "    \n",
    "    plt.style.use('fivethirtyeight')\n",
    "    plt.rcParams.update({'figure.figsize':figsize,'font.size':fontsize})\n",
    "\n",
    "    temp = df.sort_values(by = [col,hue]).copy()\n",
    "\n",
    "    p1 = sns.countplot(data = temp, x = col, hue = hue)\n",
    "\n",
    "    # annotation (absolute values)\n",
    "    for p in p1.patches: \n",
    "        if p.get_height() > 0:\n",
    "            anot = int(p.get_height())\n",
    "            p1.annotate('{:,}'.format(anot), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n",
    "\n",
    "    # annotation (share of total)\n",
    "    a = temp.groupby(col, as_index = False)[hue].count().sort_values(by = col)\n",
    "    b = np.tile(a.iloc[:,-1].to_numpy(), temp[hue].nunique())\n",
    "    zeroes = pd.crosstab(df[col],df[hue]).values.T.flatten()\n",
    "    totals = [v2 for v1,v2 in zip(zeroes,b) if v2 != 0]\n",
    "\n",
    "    for p, t in zip(p1.patches, totals):\n",
    "        if p.get_height() > 0:\n",
    "            anot = f'{round(p.get_height() / t * 100, 1)}%'\n",
    "            p1.annotate(anot, (p.get_x() + p.get_width() / 2., p.get_height()), weight = 'bold', ha = 'center', va = 'center', xytext = (0, 25), textcoords = 'offset points')        \n",
    "\n",
    "    plt.legend(bbox_to_anchor = [1.12, 0.6], title = hue)\n",
    "    plt.title(col.title().replace('_',' '), weight = 'bold', fontsize = 12, pad = 30)\n",
    "\n",
    "    p1.set_xticklabels(p1.get_xticklabels(), rotation = xticks_rot, ha = 'center')\n",
    "    p1.set(xlabel = '', ylabel = '')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5780318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, median_absolute_error\n",
    "\n",
    "to_float = lambda x: (\"%.8f\" % x).rstrip('0').rstrip('.')\n",
    "\n",
    "\"\"\"\n",
    "function's params:\n",
    "low_q         --> lower percentile for defining outliers (default value = 5%)\n",
    "high_q        --> higher percentile for defining outliers (default value = 95%)\n",
    "kde_max_q     --> max percentile for capping the actual values in the KDE plot (default value = 90%) \n",
    "max_pct_diff  --> interval's max % change between actual & predicted values\n",
    "only_pct_diff --> if the predicted value is lower than X, define intervals only by % diff.\n",
    "\"\"\"\n",
    "\n",
    "def accuracy_scores(df, actual_col, predicted_col, low_q = .05, high_q = .95, kde_max_q = .95, plots = 'yes'):\n",
    "                  \n",
    "    # print input data\n",
    "    print(f'Input dataframe length = {len(df):,}')\n",
    "    for col in [actual_col, predicted_col]:\n",
    "        length = len(df[df[col] > 0])\n",
    "        print(f'Rows with {col.upper()} = {length:,} ({round(length/len(df)*100,2)}% of input)')\n",
    "\n",
    "    # keep only rows with sufficient data    \n",
    "    x = df[(df[actual_col] > 0) & (df[predicted_col] > 0)].copy()\n",
    "    print('')\n",
    "    print(f'Rows with Sufficient Data = {len(x):,} ({round(len(x)/len(df)*100,2)}% of input)')\n",
    "       \n",
    "    # absolute error\n",
    "    x['absolute_error'] = x[actual_col] - x[predicted_col]\n",
    "    \n",
    "    # drop outliers\n",
    "    low, high = x.absolute_error.quantile(low_q), x.absolute_error.quantile(high_q)\n",
    "    x['outlier'] = np.where((x.absolute_error < low) | (x.absolute_error > high), 1, 0)\n",
    "\n",
    "    print(f'Dropped outliers [{int(low_q*100)}%,{int(high_q*100)}%] = {x.outlier.sum().astype(int):,} ({round(x.outlier.sum()/len(x)*100,2)}% of sufficient data, based on absolute error)')\n",
    "    \n",
    "    x = x[x.outlier == 0].copy()\n",
    "    print(f'Remained rows = {len(x):,}')\n",
    "    \n",
    "    print('')\n",
    "    print('-----------------------------------------------------------------------------------------------------')\n",
    "    print('R square                  | R²    = 1 - Σ(actual - predicted)^2 / Σ(actual - mean(actual))^2')\n",
    "    print('* R² is the % of variation explained by the relationship between 2 variables')           \n",
    "    print('Mean Absolute Error       | MAE   = 1/n * Σ|actual - predicted|')\n",
    "    print('Root Mean Squared Error   | RMSE  = √(1/n * Σ(actual - predicted)^2)')\n",
    "    print('Mean Abs Percentage Error | MAPE  = 100 / n * Σ[|actual - predicted| / |actual|]')\n",
    "    print('Symmetric MAPE            | SMAPE = 100 / n * Σ[2 * |actual - predicted| / (|actual| + |predicted|)]')\n",
    "    print('Median Absolute Error     | MedAE = median(|actual - predicted|)')\n",
    "    print('-----------------------------------------------------------------------------------------------------')\n",
    "    print('')\n",
    "    \n",
    "    # R-squared\n",
    "    r2 = r2_score(x[actual_col], x[predicted_col])\n",
    "       \n",
    "    # mean absolute error\n",
    "    mae = mean_absolute_error(x[actual_col], x[predicted_col])\n",
    "    \n",
    "    # root mean squared error\n",
    "    rmse = np.sqrt(mean_squared_error(x[actual_col], x[predicted_col]))\n",
    "\n",
    "    actual, forecast = x[actual_col].to_numpy(), x[predicted_col].to_numpy()\n",
    "\n",
    "    # mean absolute percentage error\n",
    "    def calc_mape(actual, forecast):\n",
    "        denominator = np.maximum(np.abs(actual), 1e-7)\n",
    "        diff = np.abs(forecast - actual)\n",
    "        return 100 / len(actual) * np.sum(diff / denominator)\n",
    "    mape = calc_mape(actual, forecast)\n",
    "    \n",
    "    # symmetric mean absolute percentage error\n",
    "    def calc_smape(actual,forecast):\n",
    "        denominator = np.maximum(np.abs(actual) + np.abs(forecast), 1e-7)\n",
    "        diff = np.abs(forecast - actual)\n",
    "        return 100 / len(actual) * np.sum(2 * diff / denominator)\n",
    "    smape = calc_smape(actual, forecast)\n",
    "    \n",
    "    # median absolute error\n",
    "    medae = median_absolute_error(x[actual_col], x[predicted_col])\n",
    "    \n",
    "    print('\\033[1m' + 'Accuracy Scores :')\n",
    "    print('\\033[0m')\n",
    "    print(f'R²    = {round(r2*100,2)}%')\n",
    "    print(f'MAE   = {round(mae,2)}')\n",
    "    print(f'RMSE  = {round(rmse,2)}')\n",
    "    print(f'MAPE  = {round(mape,2)}')\n",
    "    print(f'SMAPE = {round(smape,2)}')\n",
    "    print(f'MedAE = {round(medae,2)}')\n",
    "       \n",
    "    final = pd.DataFrame({'param':['r2','mae','rmse','mape','smape','medae'],\n",
    "                          'value':[r2,mae,rmse,mape,smape,medae]})\n",
    "    \n",
    "    print('')\n",
    "    quantiles = x[[actual_col,predicted_col,'absolute_error']].describe(percentiles = np.arange(.1,1,.1)).astype(int)\n",
    "    print(quantiles.iloc[3:,:].T)\n",
    "    \n",
    "    rcParams['figure.figsize'] = 10, 6\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    sns.set(style = 'ticks')\n",
    "    plt.style.use('seaborn-white')\n",
    "    \n",
    "    q95 = pd.concat([df[actual_col],df[predicted_col]]).quantile(0.95)\n",
    "    \n",
    "    if plots != 'no':\n",
    "        \n",
    "        kde_min = x[actual_col].quantile(kde_max_q)\n",
    "    \n",
    "        plt.figure(figsize = (8, 6)) \n",
    "        sns.set_theme(style = 'whitegrid')\n",
    "        sns.regplot(data = df[(df[actual_col] <= q95) & (df[predicted_col] <= q95)], x = 'actual', y = 'predicted', scatter = True, color = 'dodgerblue', line_kws = {'color':'orange'})\n",
    "        plt.show()\n",
    "        \n",
    "        # 2nd plot\n",
    "        plt.figure(figsize = (8, 6)) \n",
    "        colors = ['lightgrey','deepskyblue','violet']\n",
    "        k3 = sns.kdeplot(data = x[x[actual_col] < kde_min], x = 'absolute_error', lw = 3.5, color = colors[0])\n",
    "        k4 = sns.kdeplot(data = x[x[actual_col] < kde_min], x = actual_col, lw = 3.5, color = colors[1])\n",
    "        k5 = sns.kdeplot(data = x[x[actual_col] < kde_min], x = predicted_col, lw = 3.5, color = colors[2])\n",
    "        \n",
    "        patch_1 = mpatches.Patch(color = colors[1], label = actual_col.title().replace('_', ' '))\n",
    "        patch_2 = mpatches.Patch(color = colors[2], label = predicted_col.title().replace('_', ' '))\n",
    "        patch_3 = mpatches.Patch(color = colors[0], label = 'Absolute Error')\n",
    "        legend = plt.legend(handles = [patch_1,patch_2,patch_3], loc = 'upper right') \n",
    "        plt.title(f'KDE Plot (capped up to {int(kde_max_q*100)}%)', weight = 'bold', pad = 20)\n",
    "        sns.despine(bottom = True, left = True)\n",
    "        plt.show()\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f847ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution(df, col, n = 10, fig = (14,4), font = 10, sign = ''):\n",
    "    \n",
    "    plt.rcParams.update({'figure.figsize':fig, 'font.size':font})\n",
    "\n",
    "    a = df[col].describe(np.arange(1/n, 1, 1/n)).iloc[4:-1]\n",
    "    a.plot(marker = 's', ms = 7, lw = 1, color = 'dodgerblue')\n",
    "    \n",
    "    for x,y in enumerate(a):\n",
    "        plt.annotate(f'{round(y,3)}{sign}', (x,y), textcoords = 'offset points', xytext = (0,10), ha = 'center', color = 'black')\n",
    "        \n",
    "    plt.figtext(0.38, -0.04, ha = 'left', fontsize = 10, bbox = {'facecolor':'lightgrey','alpha':0.2,'pad':3,'edgecolor':'black'},\n",
    "            s = f'n = {df[col].count():,} | min = {round(df[col].min(),3):,} | max = {round(df[col].max(),3):,} | avg = {round(df[col].mean(),3):,}')\n",
    "    \n",
    "    plt.title(f\"Distribution - {col.replace('_',' ')}\", weight = 'bold', fontsize = 14)\n",
    "    plt.xlabel('')\n",
    "    sns.despine(bottom = True, left = True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8228db54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert country code to country name\n",
    "\n",
    "def country_code_to_name(code):\n",
    "    if pd.isnull(code):\n",
    "        return None\n",
    "    try:\n",
    "        return pycountry.countries.get(alpha_2 = code).name\n",
    "    except AttributeError:\n",
    "        return 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf84663-199f-4d05-a3b8-f149ec362070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_groupby(df, groupby_col, value_col, aggfunc = 'sum', threshold = .01, others_name = 'Other', \n",
    "                  dropna = False, consolidate_null = False, top_n = False):\n",
    "\n",
    "    a = df.copy()\n",
    "    gb = a.groupby(groupby_col, dropna = dropna)[value_col].agg(aggfunc).sort_values(ascending = False)\n",
    "    \n",
    "    print(f'Pre  --> {value_col} = {df[value_col].sum().astype(int):,}')\n",
    "    \n",
    "    if gb[gb.index.isnull()].shape[0] > 0:\n",
    "        print(f'Null values = {gb[gb.index.isnull()].values[0]:,}')\n",
    "    else:\n",
    "        print('Null values = 0')\n",
    "    \n",
    "    if consolidate_null:\n",
    "        a[groupby_col] = a[groupby_col].fillna(others_name)\n",
    "    \n",
    "    if isinstance(top_n,int) and top_n > 0:\n",
    "        top_values = gb.head(top_n - 1).index\n",
    "        a[groupby_col] = a[groupby_col].apply(lambda x: x if x in top_values else others_name)\n",
    "    else:\n",
    "        threshold_value = gb.sum() * threshold\n",
    "        consolidated_values = gb[gb < threshold_value].index\n",
    "        a[groupby_col] = a[groupby_col].apply(lambda x: others_name if x in consolidated_values else x)\n",
    "\n",
    "    gb2 = a.groupby(groupby_col, dropna = dropna, as_index = False)[value_col].agg(aggfunc).\\\n",
    "                    sort_values(by = value_col, ascending = False, ignore_index = True)\n",
    "    gb2['pct_total'] = gb2[value_col] / gb2[value_col].sum()\n",
    "    gb2['cumsum'] = gb2['pct_total'].cumsum()\n",
    "    \n",
    "    print(f'Post --> {value_col} = {gb2[value_col].sum().astype(int):,}'), print('')\n",
    "    \n",
    "    gb2.index += 1\n",
    "\n",
    "    return gb2.style.background_gradient(subset = 'pct_total', cmap = 'Blues', axis = None).\\\n",
    "           format({'pct_total':\"{:.2%}\",'cumsum':\"{:.2%}\"}, precision = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb5af13-35b1-44aa-bcc3-eca1839fd363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
